{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from math import sqrt\n",
        "import os\n",
        "from torch.nn.utils import weight_norm\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import collections\n",
        "import numbers\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "from torch.autograd import Variable\n",
        "import scipy.misc\n",
        "import time"
      ],
      "metadata": {
        "id": "yK1Bu8zTp9Yp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cO3r4wj_o5hn"
      },
      "outputs": [],
      "source": [
        "# Embed.py \n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, dropout=0.0):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attn.py\n",
        "class TriangularCausalMask():\n",
        "    def __init__(self, B, L, device=\"cpu\"):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask\n",
        "\n",
        "\n",
        "class AnomalyAttention(nn.Module):\n",
        "    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n",
        "        super(AnomalyAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "        window_size = win_size\n",
        "        self.distances = torch.zeros((window_size, window_size)).cuda()\n",
        "        for i in range(window_size):\n",
        "            for j in range(window_size):\n",
        "                self.distances[i][j] = abs(i - j)\n",
        "\n",
        "    def forward(self, queries, keys, values, sigma, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1. / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "        attn = scale * scores\n",
        "\n",
        "        sigma = sigma.transpose(1, 2)  # B L H ->  B H L\n",
        "        window_size = attn.shape[-1]\n",
        "        sigma = torch.sigmoid(sigma * 5) + 1e-5\n",
        "        sigma = torch.pow(3, sigma) - 1\n",
        "        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)  # B H L L\n",
        "        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda()\n",
        "        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))\n",
        "\n",
        "        series = self.dropout(torch.softmax(attn, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), series, prior, sigma)\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
        "                 d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model,\n",
        "                                          d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model,\n",
        "                                        d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model,\n",
        "                                          d_values * n_heads)\n",
        "        self.sigma_projection = nn.Linear(d_model,\n",
        "                                          n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "        x = queries\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "        sigma = self.sigma_projection(x).view(B, L, H)\n",
        "\n",
        "        out, series, prior, sigma = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values,\n",
        "            sigma,\n",
        "            attn_mask\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), series, prior, sigma\n"
      ],
      "metadata": {
        "id": "t6tvBclUo7PX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anomaly Transformer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn, mask, sigma = self.attention(\n",
        "            x, x, x,\n",
        "            attn_mask=attn_mask\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm2(x + y), attn, mask, sigma\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x [B, L, D]\n",
        "        series_list = []\n",
        "        prior_list = []\n",
        "        sigma_list = []\n",
        "        for attn_layer in self.attn_layers:\n",
        "            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)\n",
        "            series_list.append(series)\n",
        "            prior_list.append(prior)\n",
        "            sigma_list.append(sigma)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, series_list, prior_list, sigma_list\n",
        "\n",
        "\n",
        "class AnomalyTransformer(nn.Module):\n",
        "    def __init__(self, win_size, enc_in, c_out, d_model=512, n_heads=8, e_layers=3, d_ff=512,\n",
        "                 dropout=0.0, activation='gelu', output_attention=True):\n",
        "        super(AnomalyTransformer, self).__init__()\n",
        "        self.output_attention = output_attention\n",
        "\n",
        "        # Encoding\n",
        "        self.embedding = DataEmbedding(enc_in, d_model, dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        AnomalyAttention(win_size, False, attention_dropout=dropout, output_attention=output_attention),\n",
        "                        d_model, n_heads),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation\n",
        "                ) for l in range(e_layers)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_out = self.embedding(x)\n",
        "        enc_out, series, prior, sigmas = self.encoder(enc_out)\n",
        "        enc_out = self.projection(enc_out)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return enc_out, series, prior, sigmas\n",
        "        else:\n",
        "            return enc_out  # [B, L, D]\n"
      ],
      "metadata": {
        "id": "lDBNiBBFo7XR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils.py\n",
        "def to_var(x, volatile=False):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, volatile=volatile)\n",
        "\n",
        "\n",
        "def mkdir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n"
      ],
      "metadata": {
        "id": "LV6USgY6o7er"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils/Logger.p\n",
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO  # Python 3.5+\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values ** 2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n"
      ],
      "metadata": {
        "id": "GyvEkkgIo7h8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data_Loader \n",
        "class SMDSegLoader(object):\n",
        "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        self.win_size = win_size\n",
        "        self.scaler = StandardScaler()\n",
        "        data = np.load(data_path + \"/SMD_train.npy\")\n",
        "        self.scaler.fit(data)\n",
        "        data = self.scaler.transform(data)\n",
        "        test_data = np.load(data_path + \"/SMD_test.npy\")\n",
        "        self.test = self.scaler.transform(test_data)\n",
        "        self.train = data\n",
        "        data_len = len(self.train)\n",
        "        self.val = self.train[(int)(data_len * 0.8):]\n",
        "        self.test_labels = np.load(data_path + \"/SMD_test_label.npy\")\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'val'):\n",
        "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
        "        elif (self.mode == 'test'):\n",
        "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
        "        else:\n",
        "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.step\n",
        "        if self.mode == \"train\":\n",
        "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'val'):\n",
        "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
        "        elif (self.mode == 'test'):\n",
        "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
        "                self.test_labels[index:index + self.win_size])\n",
        "        else:\n",
        "            return np.float32(self.test[\n",
        "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
        "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n",
        "\n",
        "\n",
        "def get_loader_segment(data_path, batch_size, win_size=100, step=100, mode='train', dataset='KDD'):\n",
        "    if (dataset == 'SMD'):\n",
        "        dataset = SMDSegLoader(data_path, win_size, step, mode)\n",
        "    elif (dataset == 'MSL'):\n",
        "        dataset = MSLSegLoader(data_path, win_size, 1, mode)\n",
        "    elif (dataset == 'SMAP'):\n",
        "        dataset = SMAPSegLoader(data_path, win_size, 1, mode)\n",
        "    elif (dataset == 'PSM'):\n",
        "        dataset = PSMSegLoader(data_path, win_size, 1, mode)\n",
        "\n",
        "    shuffle = False\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=0)\n",
        "    return data_loader\n"
      ],
      "metadata": {
        "id": "j6Vm3o4PrBA2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_kl_loss(p, q):\n",
        "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
        "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr_):\n",
        "    lr_adjust = {epoch: lr_ * (0.5 ** ((epoch - 1) // 1))}\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, dataset_name='', delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.best_score2 = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_loss2_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.dataset = dataset_name\n",
        "\n",
        "    def __call__(self, val_loss, val_loss2, model, path):\n",
        "        score = -val_loss\n",
        "        score2 = -val_loss2\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "        elif score < self.best_score + self.delta or score2 < self.best_score2 + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_score2 = score2\n",
        "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, val_loss2, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), os.path.join(path, str(self.dataset) + '_checkpoint.pth'))\n",
        "        self.val_loss_min = val_loss\n",
        "        self.val_loss2_min = val_loss2\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    DEFAULTS = {}\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
        "\n",
        "        self.train_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                               mode='train',\n",
        "                                               dataset=self.dataset)\n",
        "        self.vali_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='val',\n",
        "                                              dataset=self.dataset)\n",
        "        self.test_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='test',\n",
        "                                              dataset=self.dataset)\n",
        "        self.thre_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
        "                                              mode='thre',\n",
        "                                              dataset=self.dataset)\n",
        "\n",
        "        self.build_model()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.num_epochs = config['num_epochs']\n",
        "        self.k = config['k']\n",
        "        self.anormly_ratio= config['anomaly_ratio']\n",
        "\n",
        "    def build_model(self):\n",
        "        self.model = AnomalyTransformer(win_size=self.win_size, enc_in=self.input_c, c_out=self.output_c, e_layers=3)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "\n",
        "    def vali(self, vali_loader):\n",
        "        self.model.eval()\n",
        "\n",
        "        loss_1 = []\n",
        "        loss_2 = []\n",
        "        for i, (input_data, _) in enumerate(vali_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                               self.win_size)).detach())) + torch.mean(\n",
        "                    my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)).detach(),\n",
        "                        series[u])))\n",
        "                prior_loss += (torch.mean(\n",
        "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)),\n",
        "                               series[u].detach())) + torch.mean(\n",
        "                    my_kl_loss(series[u].detach(),\n",
        "                               (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "            series_loss = series_loss / len(prior)\n",
        "            prior_loss = prior_loss / len(prior)\n",
        "\n",
        "            rec_loss = self.criterion(output, input)\n",
        "            loss_1.append((rec_loss - self.k * series_loss).item())\n",
        "            loss_2.append((rec_loss + self.k * prior_loss).item())\n",
        "\n",
        "        return np.average(loss_1), np.average(loss_2)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        print(\"======================TRAIN MODE======================\")\n",
        "\n",
        "        time_now = time.time()\n",
        "        path = self.model_save_path\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        early_stopping = EarlyStopping(patience=3, verbose=True, dataset_name=self.dataset)\n",
        "        train_steps = len(self.train_loader)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iter_count = 0\n",
        "            loss1_list = []\n",
        "\n",
        "            epoch_time = time.time()\n",
        "            self.model.train()\n",
        "            for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                iter_count += 1\n",
        "                input = input_data.float().to(self.device)\n",
        "\n",
        "                output, series, prior, _ = self.model(input)\n",
        "\n",
        "                # calculate Association discrepancy\n",
        "                series_loss = 0.0\n",
        "                prior_loss = 0.0\n",
        "                for u in range(len(prior)):\n",
        "                    series_loss += (torch.mean(my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach())) + torch.mean(\n",
        "                        my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                           self.win_size)).detach(),\n",
        "                                   series[u])))\n",
        "                    prior_loss += (torch.mean(my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach())) + torch.mean(\n",
        "                        my_kl_loss(series[u].detach(), (\n",
        "                                prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                       self.win_size)))))\n",
        "                series_loss = series_loss / len(prior)\n",
        "                prior_loss = prior_loss / len(prior)\n",
        "\n",
        "                rec_loss = self.criterion(output, input)\n",
        "\n",
        "                loss1_list.append((rec_loss - self.k * series_loss).item())\n",
        "                loss1 = rec_loss - self.k * series_loss\n",
        "                loss2 = rec_loss + self.k * prior_loss\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    speed = (time.time() - time_now) / iter_count\n",
        "                    left_time = speed * ((self.num_epochs - epoch) * train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                # Minimax strategy\n",
        "                loss1.backward(retain_graph=True)\n",
        "                loss2.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "            train_loss = np.average(loss1_list)\n",
        "\n",
        "            vali_loss1, vali_loss2 = self.vali(self.test_loader)\n",
        "\n",
        "            print(\n",
        "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} \".format(\n",
        "                    epoch + 1, train_steps, train_loss, vali_loss1))\n",
        "            early_stopping(vali_loss1, vali_loss2, self.model, path)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "            adjust_learning_rate(self.optimizer, epoch + 1, self.lr)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.load_state_dict(\n",
        "            torch.load(\n",
        "                os.path.join(str(self.model_save_path), str(self.dataset) + '_checkpoint.pth')))\n",
        "        self.model.eval()\n",
        "        temperature = 50\n",
        "\n",
        "        print(\"======================TEST MODE======================\")\n",
        "\n",
        "        criterion = nn.MSELoss(reduce=False)\n",
        "\n",
        "        # (1) stastic on the train set\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.train_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        train_energy = np.array(attens_energy)\n",
        "\n",
        "        # (2) find the threshold\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            # Metric\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
        "        thresh = np.percentile(combined_energy, 100 - self.anormly_ratio)\n",
        "        print(\"Threshold :\", thresh)\n",
        "\n",
        "        # (3) evaluation on the test set\n",
        "        test_labels = []\n",
        "        attens_energy = []\n",
        "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
        "            input = input_data.float().to(self.device)\n",
        "            output, series, prior, _ = self.model(input)\n",
        "\n",
        "            loss = torch.mean(criterion(input, output), dim=-1)\n",
        "\n",
        "            series_loss = 0.0\n",
        "            prior_loss = 0.0\n",
        "            for u in range(len(prior)):\n",
        "                if u == 0:\n",
        "                    series_loss = my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss = my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "                else:\n",
        "                    series_loss += my_kl_loss(series[u], (\n",
        "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                   self.win_size)).detach()) * temperature\n",
        "                    prior_loss += my_kl_loss(\n",
        "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
        "                                                                                                self.win_size)),\n",
        "                        series[u].detach()) * temperature\n",
        "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
        "\n",
        "            cri = metric * loss\n",
        "            cri = cri.detach().cpu().numpy()\n",
        "            attens_energy.append(cri)\n",
        "            test_labels.append(labels)\n",
        "\n",
        "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
        "        test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
        "        test_energy = np.array(attens_energy)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "        pred = (test_energy > thresh).astype(int)\n",
        "\n",
        "        gt = test_labels.astype(int)\n",
        "\n",
        "        print(\"pred:   \", pred.shape)\n",
        "        print(\"gt:     \", gt.shape)\n",
        "\n",
        "        # detection adjustment\n",
        "        anomaly_state = False\n",
        "        for i in range(len(gt)):\n",
        "            if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
        "                anomaly_state = True\n",
        "                for j in range(i, 0, -1):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "                for j in range(i, len(gt)):\n",
        "                    if gt[j] == 0:\n",
        "                        break\n",
        "                    else:\n",
        "                        if pred[j] == 0:\n",
        "                            pred[j] = 1\n",
        "            elif gt[i] == 0:\n",
        "                anomaly_state = False\n",
        "            if anomaly_state:\n",
        "                pred[i] = 1\n",
        "\n",
        "        pred = np.array(pred)\n",
        "        gt = np.array(gt)\n",
        "        print(\"pred: \", pred.shape)\n",
        "        print(\"gt:   \", gt.shape)\n",
        "\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        accuracy = accuracy_score(gt, pred)\n",
        "        precision, recall, f_score, support = precision_recall_fscore_support(gt, pred,\n",
        "                                                                              average='binary')\n",
        "        print(\n",
        "            \"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} \".format(\n",
        "                accuracy, precision,\n",
        "                recall, f_score))\n",
        "\n",
        "        return accuracy, precision, recall, f_score\n"
      ],
      "metadata": {
        "id": "2jcdh78qo7lD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "python main.py --anormly_ratio 0.5 --num_epochs 10   --batch_size 256  --mode train --dataset SMD  --data_path dataset/SMD   --input_c 38\n",
        "python main.py --anormly_ratio 0.5 --num_epochs 10   --batch_size 256     --mode test    --dataset SMD   --data_path dataset/SMD     --input_c 38     --pretrained_model 20"
      ],
      "metadata": {
        "id": "xVNSNFBz4isf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "anormly_ratio: 0.5\n",
        "batch_size: 256\n",
        "data_path: dataset/SMD\n",
        "dataset: SMD\n",
        "input_c: 38\n",
        "k: 3\n",
        "lr: 0.0001\n",
        "mode: test\n",
        "model_save_path: checkpoints\n",
        "num_epochs: 10\n",
        "output_c: 38\n",
        "pretrained_model: 20\n",
        "win_size: 100"
      ],
      "metadata": {
        "id": "02FpC59b59u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smd_data = SMDSegLoader('/content/drive/MyDrive/SMD', win_size = 100, step = 100)\n"
      ],
      "metadata": {
        "id": "WRf3IhrA4jYS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd_data_loader = get_loader_segment(data_path = '/content/drive/MyDrive/SMD', batch_size=256, win_size=100, step=100, mode='train', dataset='SDM')"
      ],
      "metadata": {
        "id": "eBJXciuV5W5L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_i = next(iter(smd_data_loader))"
      ],
      "metadata": {
        "id": "3Gg2hAln9SmL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dBB5EaK9odJ",
        "outputId": "7b05ee48-51f0-4eb2-ec5f-02a8f44132d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['D', 'M', 'S']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = { 'lr' : 1e-4, 'win_size': 100, 'output_c': 38, 'input_c': 38, 'batch_size' : 256, 'pretrained_model': 20, 'mode' : 'train',\n",
        "          'dataset': 'SMD', 'data_path': '/content/drive/MyDrive/SMD', 'model_save_path' : 'Checkpoints', 'anomaly_ratio': 0.5, 'k': 3 , 'num_epochs' : 10\n",
        "    \n",
        "}"
      ],
      "metadata": {
        "id": "QL6xGkG0E7sN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slover = Solver(config)"
      ],
      "metadata": {
        "id": "nq7sxxaME-cf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slover.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRP1my-hIMkE",
        "outputId": "17cd14a1-4098-42f5-de16-41007c16d060"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================TRAIN MODE======================\n",
            "Epoch: 1 cost time: 24.89627981185913\n",
            "Epoch: 1, Steps: 28 | Train Loss: -21.6170094 Vali Loss: -22.9663467 \n",
            "Validation loss decreased (inf --> -22.966347).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 19.082749366760254\n",
            "Epoch: 2, Steps: 28 | Train Loss: -32.3343212 Vali Loss: -39.7256244 \n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 20.093801259994507\n",
            "Epoch: 3, Steps: 28 | Train Loss: -42.4404721 Vali Loss: -42.7454228 \n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 20.382895469665527\n",
            "Epoch: 4, Steps: 28 | Train Loss: -44.2640751 Vali Loss: -43.5678099 \n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slover.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5mHirqKIurZ",
        "outputId": "4561e318-65f9-4ecd-a5a6-54f1bba7b654"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================TEST MODE======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold : 0.12872106216847823\n",
            "pred:    (708400,)\n",
            "gt:      (708400,)\n",
            "pred:  (708400,)\n",
            "gt:    (708400,)\n",
            "Accuracy : 0.9920, Precision : 0.8891, Recall : 0.9218, F-score : 0.9052 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.991972049689441, 0.8891400491400492, 0.9217837250373591, 0.9051676699628141)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F77GogtwSWaK"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}